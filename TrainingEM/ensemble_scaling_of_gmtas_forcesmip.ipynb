{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd25e7bc-8ed7-4cdd-8783-9e28c5a76c4c",
   "metadata": {},
   "source": [
    "# Multi-model mean scaling\n",
    "\n",
    "This notebook takes the multimodel mean forced climate change (from the training dataset) and scales it to match a simulation / dataset of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d410a5-bae9-4635-b05c-63b2ac27dfab",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f10429-4098-4f4e-9ac5-9c6e380661f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xcdat as xc\n",
    "from fx import verify_monthly_times_match\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "from scipy.signal import savgol_filter\n",
    "# suppress warnings (but allow errors)\n",
    "import logging\n",
    "logging.getLogger('xcdat').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5122266-022e-4506-8699-c50f45b6a2e7",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da4c475-bd4d-4df5-aa86-cf5bbf70ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_multimodel_mean_timeseries(ds_raw, reference_data, vmap):\n",
    "    \"\"\"\n",
    "    scale_multimodel_mean_timeseries(ds_raw, reference_data, vmap)\n",
    "\n",
    "    Function solves for the best scaling of a multimodel mean, global mean\n",
    "    surface air temperature time series (forced response) and a \"raw\" global\n",
    "    mean time series that includes both internal variability and the forced\n",
    "    response such that:\n",
    "\n",
    "        y = m*x + b\n",
    "\n",
    "    where y is the smoothed global mean \"raw\" surface temperature time \n",
    "    series, x is the smoother global average reference surface temperature\n",
    "    time series, and m and b are the fitted slope and intercept,\n",
    "    respectively. The slope is then applied to the reference timeseries of a\n",
    "    field of interest (e.g., sea ice concentration, surface pressure, or\n",
    "    atmospheric temperature) to estimate the forced time series in the raw\n",
    "    dataset such that:\n",
    "        \n",
    "        F[t, x, y] = m*X[t, x, y]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds_raw (xr.Dataset)      : Dataset containing the raw (forced+internal)\n",
    "                               surface temperature data (contains \"tas\")\n",
    "    reference_data (Dict)    : Dictionary containing reference datasets to\n",
    "                               be scaled in form of reference_data[variable_name] = ds\n",
    "    vmap (str)               : Mapping for variable name, cmipTable, and netcdf id in form\n",
    "                               vmap[variable_name] = [cmipTable, netcdf_id] or\n",
    "                               vmap['zmta'] = ['Amon', 'ta']\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dict : Dict containing datasets of the forced response for each field, e.g.,\n",
    "           scaled_data['tas'] = ds\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    Smoothing is performed using a third order Savitzky-Golay filter (1) with a window length\n",
    "    of 120 months. For 3D atmospheric temperature data (plev=50000) is chosen in order to fit\n",
    "    the scaling coefficients. If a missing value exists in either the reference or raw data\n",
    "    then a missing value is included in both datasets.\n",
    "    \n",
    "    [1] https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html\n",
    "    \"\"\"\n",
    "    # get reference tas data for scaling\n",
    "    ds_ref_tas = reference_data['tas'].copy()\n",
    "    # subset time axis if needed\n",
    "    if len(ds_ref_tas.time) != len(ds_raw.time):\n",
    "        subsetTime = True\n",
    "        rtime = ds_raw.time.values\n",
    "        startString = str(rtime[0].year) + '-' + str(rtime[0].month).zfill(2)\n",
    "        endString = str(rtime[-1].year) + '-' + str(rtime[-1].month).zfill(2)\n",
    "        timeSlice = slice(startString, endString)\n",
    "        ds_ref_tas = ds_ref_tas.sel(time=timeSlice)\n",
    "    else:\n",
    "        subsetTime = False\n",
    "    # time axis match\n",
    "    if verify_monthly_times_match(ds_ref_tas.time, ds_raw.time):\n",
    "        ds_raw['time'] = ds_ref_tas['time']\n",
    "    else:\n",
    "        raise ValueError('Time axes do not match.')\n",
    "    # masking of global mean surface temperature time series\n",
    "    ds_ref_tas[\"tas\"] = xr.where(~np.isnan(ds_raw[\"tas\"]), ds_ref_tas[\"tas\"], np.nan)\n",
    "    ds_raw[\"tas\"] = xr.where(~np.isnan(ds_ref_tas[\"tas\"]), ds_raw[\"tas\"], np.nan)\n",
    "    # add missing bounds (if needed)\n",
    "    ds_ref_tas = ds_ref_tas.bounds.add_missing_bounds(['X', 'Y', 'T'])\n",
    "    ds_raw = ds_raw.bounds.add_missing_bounds(['X', 'Y', 'T'])\n",
    "    # compute global mean average\n",
    "    ds_ref_tas_gm = ds_ref_tas.spatial.average('tas')\n",
    "    ds_raw_gm = ds_raw.spatial.average('tas')\n",
    "    ts_ref_tas_gm = ds_ref_tas_gm.temporal.departures('tas', freq='month')['tas']\n",
    "    ts_raw_gm = ds_raw_gm.temporal.departures('tas', freq='month')['tas']\n",
    "    # smooth the \"raw\" data\n",
    "    ts_raw_gm_smooth = ts_raw_gm.copy()\n",
    "    ts_raw_gm_smooth.values = savgol_filter(ts_raw_gm, 10*12, 3)\n",
    "    # do fit\n",
    "    m, b = np.polyfit(ts_ref_tas_gm, ts_raw_gm_smooth, 1)\n",
    "    # loop over and scale all reference fields\n",
    "    scaled_data = {}\n",
    "    for vid in reference_data.keys():\n",
    "        # get netcdf id to read data\n",
    "        lvar = vmap[vid][1]\n",
    "        # copy reference data before manipulation\n",
    "        ds_ref = reference_data[vid].copy()\n",
    "        if subsetTime:\n",
    "            ds_ref = ds_ref.sel(time=timeSlice)\n",
    "        # copy reference dataset to use for output data\n",
    "        ds_fitted = ds_ref.copy()\n",
    "        # scale reference dataarray and insert into output dataset\n",
    "        ds_fitted[lvar] = ds_ref[lvar] * m\n",
    "        # add output dataset to scaled_data dictionary\n",
    "        scaled_data[vid] = ds_fitted.copy()\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32c7a5-66c4-47d5-a6d1-a7108fb67c29",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee6901c-9578-4d3a-81e6-c3106856b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "forcesmip_root = '/glade/campaign/cgd/cas/asphilli/ForceSMIP/'\n",
    "dpath_em = '/glade/work/pochedls/forcesmip/ensemble_mean/'\n",
    "dpath_out = '/glade/work/pochedls/forcesmip/'\n",
    "fmethod = 'scaleMultiModelMeanBasedOnGlobalTas'\n",
    "vmap = {'pr': ['Amon', 'pr'],\n",
    "        'psl': ['Amon', 'psl'],\n",
    "        'tas': ['Amon', 'tas'],\n",
    "        'zmta': ['Amon', 'ta'],\n",
    "        'monmaxpr': ['Aday', 'pr'],\n",
    "        'monmaxtasmax': ['Aday', 'tasmax'],\n",
    "        'monmintasmin': ['Aday', 'tasmin'],\n",
    "        'siconc': ['OImon', 'siconc'],\n",
    "        'tos': ['Omon', 'tos']}\n",
    "models = ['CanESM5', 'CESM2', 'MIROC6', 'MIROC-ES2L', 'MPI-ESM1-2-LR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375527db-4d46-47cb-8bba-a36739eb8ac3",
   "metadata": {},
   "source": [
    "### Load Reference Multimodel Mean (forced) Time Series for Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654ca134-44c3-4674-9cd7-577ff942de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mapping for reference variable files\n",
    "reference_data = {}\n",
    "for vid in vmap.keys():\n",
    "    # get CMIP table\n",
    "    cmipTable = vmap[vid][0]\n",
    "    # get appropriate netcdf variable id\n",
    "    lvar = vmap[vid][1]\n",
    "    # open reference datasets\n",
    "    fnr = dpath_em + vid + '_mon_MMM_historical_ssp370_ensmean.nc'\n",
    "    ds = xc.open_dataset(fnr, add_bounds=[\"T\", \"X\", \"Y\"])\n",
    "    ds = ds.temporal.departures(lvar, freq='month')\n",
    "    reference_data[vid]= ds.load()\n",
    "    ds.close()\n",
    "\n",
    "# ensure all time axes match\n",
    "for vid in vmap.keys():\n",
    "    if not verify_monthly_times_match(reference_data['tas'].time, reference_data[vid].time):\n",
    "        raise ValueError('Reference time axes do not match.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b944f1-cf72-4416-8ca3-d891db29cd35",
   "metadata": {},
   "source": [
    "### Estimate Forced Response in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094ef631-e4df-419d-b76b-a577f2d811f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CanESM5\n",
      "   CESM2\n",
      "   MIROC6\n",
      "   MIROC-ES2L\n",
      "   MPI-ESM1-2-LR\n"
     ]
    }
   ],
   "source": [
    "# first ensure output path exists\n",
    "if not os.path.exists(dpath_out + '/training_predictions/'):\n",
    "    os.makedirs(dpath_out + '/training_predictions/')\n",
    "\n",
    "# open reference global mean surface temperature dataset\n",
    "fnrtas = dpath_em + 'tas_mon_MMM_historical_ssp370_ensmean.nc'\n",
    "ds_ref_tas = xc.open_dataset(fnrtas)\n",
    "for model in models:\n",
    "    # print progress\n",
    "    print('   ' + model)\n",
    "    # specify data path\n",
    "    dpath = forcesmip_root + '/Training/Amon/tas/' + model\n",
    "    # get all files for model\n",
    "    mfiles = glob.glob(dpath + '/*nc')\n",
    "    # loop over all files / members\n",
    "    for fn in mfiles:\n",
    "        # get member\n",
    "        if model == 'CESM2':\n",
    "            member = '.'.join(fn.split('_')[-1].split('.')[0:2])\n",
    "        else:\n",
    "            member = fn.split('.')[0].split('_')[-1]\n",
    "        # open dataset\n",
    "        ds_raw = xc.open_dataset(fn)\n",
    "        # do fit\n",
    "        scaled_data = scale_multimodel_mean_timeseries(ds_raw, reference_data, vmap)\n",
    "        # save out each scaled dataset/variable\n",
    "        for vid in scaled_data.keys():\n",
    "            # specify output path\n",
    "            fnOut = dpath_out + '/training_predictions/' + vid + '_mon_' + model + '_' + fmethod + '_historical_ssp370_' + member + '.' + mfiles[0].split('.')[-1]\n",
    "            # get scaled data\n",
    "            ds = scaled_data[vid]\n",
    "            # save output\n",
    "            ds.to_netcdf(fnOut)\n",
    "        # close file\n",
    "        ds_raw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159963fb-7b07-49cc-8100-65caf6347e3b",
   "metadata": {},
   "source": [
    "### Estimate Forced Response in Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de30fb9-78a1-49b0-a388-24db6c3a4aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1F\n",
      "   1I\n",
      "   1E\n",
      "   1G\n",
      "   1D\n",
      "   1B\n",
      "   1A\n",
      "   1C\n",
      "   1H\n",
      "   1J\n"
     ]
    }
   ],
   "source": [
    "# first ensure output path exists\n",
    "if not os.path.exists(dpath_out + '/evaluation_predictions/'):\n",
    "    os.makedirs(dpath_out + '/evaluation_predictions/')\n",
    "\n",
    "# get evaluation files\n",
    "mfiles = glob.glob(forcesmip_root + '/Evaluation-Tier1/Amon/tas/*nc')\n",
    "# loop over all evaluation models\n",
    "for fn in mfiles:\n",
    "    # specify output path\n",
    "    member = fn.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    print('   ' + member)\n",
    "    # open dataset\n",
    "    ds_raw = xc.open_dataset(fn)\n",
    "    # do fit\n",
    "    scaled_data = scale_multimodel_mean_timeseries(ds_raw, reference_data, vmap)\n",
    "    # save out each scaled dataset/variable\n",
    "    for vid in scaled_data.keys():\n",
    "        # specify output path\n",
    "        fnOut = dpath_out + '/evaluation_predictions/' + vid + '_' + member + '_tier1_' + fmethod + '_benchmark.nc'\n",
    "        # get scaled data\n",
    "        ds = scaled_data[vid]\n",
    "        # save output\n",
    "        ds.to_netcdf(fnOut)\n",
    "    # close file\n",
    "    ds_raw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcdat",
   "language": "python",
   "name": "xcdat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
